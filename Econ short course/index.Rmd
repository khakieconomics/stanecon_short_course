--- 
title: "A brief introduction to econometrics in Stan"
author: "James Savage"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
    bookdown::gitbook: default
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: khakieconomist/BSEcon
description: "This book provides an introduction to Bayesian modeling, and examples of the common techniques used in many fields of econometrics."
---

# About {-}

These notes are for a one-day short course in econometrics using Stan. The main reason to learn
Stan is to fit models that are difficult to fit using other software. Such models might include
models with high-dimensional random effects (about which we want to draw inference), models 
with complex or multi-stage likelihoods, or models with latent data structures. A second reason
to learn Stan is that you want to conduct Bayesian analysis on workhorse models; perhaps you have
good prior information, or are attracted to the possibility of making probabilistic statements about
predictions and parameter estimates. 

While this second reason is worthwhile, it is not the aim of this course. This course introduces 
a few workhorse models in order to give you the skills to build richer models that extract the most
information from your data. There are three sessions: 

1. An introduction to Modern Statistical Workflow, using an instrumental variables model as the example. We will also touch on Simultaneous Equations Modeling. 
2. Hierarchical models and hierarchical priors, of which we can consider panel data a special case. We'll cover fixed and random effects, post-stratification, and the Gelman-Bafumi correction.
3. An introduction to time-series models, including time-varying parameters, latent factor models, and structural VARs. 

These notes have a few idiosyncracies: 

> Tricks and shortcuts will look like this

The code examples live in the `models/` folder of the book's repository, (https://github.com/khakieconomics/shortcourse/models). 

We use two computing languages in these notes. The first is Stan, a powerful modeling language
that allows us to express and estimate probabilistic models with continuous parameter spaces. 
Stan programs are prefaced with their location in the `models/` folder, like so:

```
// models/model_1.stan
// ...  model code here
```

We also use the `R` language, for data preparation, calling Stan models, and visualising model
results. R programs live in the `scripts/` folder; they typically read data from the `data/` folder, 
and liberally use `magrittr` syntax with `dplyr`. If this syntax is unfamiliar to you, it is worth
taking a look at the [excellent vignette](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html)
to the `dplyr` package. Like the Stan models, all R code in the book is prefaced with its location
in the book's directory. 

```{r}
# scripts/intro.R
# ... data work here
```

It is not necessary to be an R aficionado to make the most of these notes. Stan programs can be called from within
Stata, Matlab, Mathematica, Julia and Python. If you are more comfortable using those languages than R for data
preparation work, then you should be able to implement all the models in this book using those interfaces. Further
documentation on calling Stan from other environments is available at [http://mc-stan.org/interfaces/](http://mc-stan.org/interfaces/).

While Stan can be called quite easily from these other programming environments, the R implementation is more 
fully-fleshed---especially for model checking and post-processing. For this reason we use the R implementation
of Stan, `rstan` in this book.

## The structure {-}

An important premise in these is that we should only build richer, more complex models
when simple ones will not do. After explaining the necessary preliminary concepts, 
Each session is set up around this theme. 

The first session offers an introduction to Stan, walking you through the steps of building,
estimating, and checking a probability model. We call this procedure _Modern Statistical Workflow_, and
recommend it be followed for essentially all modeling tasks. If you're an experienced modeler 
and understand the preliminaries already, this is a good place to start. 

The second session covers hierarchical modeling. The central notion in hierarchical modeling
is that our data has some hierarchy. Some examples might illustrate the idea: 

- Our observations are noisy measures of some true value, about which we want to infer. 
- We have multiple observations from many administrative units, for example students within a school within a region. 
- We observe many individuals over time (panel data). 

There is a large cultural difference between panel/hierarchical data as used by econometricians and 
as used by Bayesian statisticians. We'll take a more statistical approach in this book. The big 
difference is that Bayesian statisticians think that the primary goal of using hierarchical data 
is to fit a model _at the level of the individual_, but recognising that information from other 
individuals might be useful in estimating that model. It's a crass simplification, but economists
tend to view the goal of using panel data as helping to estimate an unbiased or less biased treatment
effect that abstracts from unobserved information fixed within the individual. These are different goals, 
and we will discuss them later.  

We will cover fixed and random effects, and the Gelman-Bafumi correction (which makes random effects models
more widely applicable). We also discuss how to incorporate instruments in these models. 

The last session introduces some fun time-series models. Chapter seven illustrates how to implement more 
advanced multivariate time-series models. These include Structural Vector Autoregressions (SVAR), factor models, and state-space methods, including time-varying parameter regressions, and low-to-high frequency missing values interpolation. 

### A note on data

Through this short course, we will not use any real data, but rather force you to simulate fake data where 
the "unknowns are known". This is very good practice, both from the perspective of model checking, but also 
helping you to understand the underlying data generating process that you are trying to model. 
